{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST 이미지로 CNN 구현\n",
    "###### 코드 설명이 되어있는 링크 https://medium.com/@inmoonlight/pytorch%EB%A1%9C-%EB%94%A5%EB%9F%AC%EB%8B%9D%ED%95%98%EA%B8%B0-cnn-62a9326111ae\n",
    "###### cnn 설명이 잘 되어있는 링크 https://medium.com/@hobinjeong/cnn-convolutional-neural-network-9f600dd3b395\n",
    "###### cnn 설명이 잘 되어있는 링크 https://mc.ai/cnn%EC%97%90%EC%84%9C-pooling%EC%9D%B4%EB%9E%80/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.Compose([\n",
    "                                 transforms.ToTensor(), # image to Tensor\n",
    "                                 transforms.Normalize((0.1307,), (0.3081,)) # image, label\n",
    "                             ]))\n",
    "\n",
    "# trainloader = torch.utils.data.DataLoader(mnist_trainset, batch_size=4,\n",
    "#                                           shuffle=True, num_workers=2,\n",
    "#                                          )\n",
    "\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True,  transform=transforms.Compose([\n",
    "                                 transforms.ToTensor(), # image to Tensor\n",
    "                                 transforms.Normalize((0.1307,), (0.3081,)) # image, label\n",
    "                             ]))\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "trainloader = torch.utils.data.DataLoader(mnist_trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2,\n",
    "                                         )\n",
    "\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(mnist_testset,batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전처리 과정에 대해\n",
    "\n",
    "\n",
    "\n",
    "##### *num_worders 는 thread 수\n",
    "\n",
    "### 정리하자면 원래 이미지는 [0,255] torchvision은  [0,1]의 범위로 만들고 이를 [-1,1]로 한번더 바꿔 주었다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 이미지를 보여주기 위한 함수\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize([0,1]로 범위를 바꿔줌)\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\n",
    "# 학습용 이미지를 무작위로 가져오기\n",
    "dataiter = iter(mnist_trainset)\n",
    "images, labels = next(dataiter);\n",
    "\n",
    "# 이미지 보여주기\n",
    "imshow(torchvision.utils.make_grid(images))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 용어 설명\n",
    "#### convolution : 3*3 같은 크기로 (도장처럼) 처리하기 - > 3*3의 가중치가 있고 (activtion(가중치*각 픽셀))의 과정\n",
    "#### padding : 크기가 작아진 이미지에 padding을 더하기\n",
    "#### pooling : 특징 뽑아내기(크기가 작아진다)(max pooling, avg pooling 등이 있고 과적합을 막는다)\n",
    "#### 다시 원래의 이미지 크기로 확대\n",
    "\n",
    "#### output 채널의 계산\n",
    "\n",
    "    Total size ; N (i.e. N x N)\n",
    "\n",
    "    Filter size ; F (i.e. F x F)\n",
    "\n",
    "    Output size ; k:=(N-F)/stride + 1 where k is an integer.\n",
    "    예를들어 N =7*7 F = 3*3 stride= 1 이면 outputsize는(7-3)/1 +1 =5\n",
    "    \n",
    "#### reshape 함수 https://stackoverflow.com/questions/42479902/how-does-the-view-method-work-in-pytorch\n",
    "        array의 행과 열을 변형해준다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv2d\n",
    "\n",
    "#### class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
    "\n",
    "Conv2d의 parameters는 차례대로 in_channels, out_channels, kernel_size, stride, padding, diction , groups, bias 가 있다.\n",
    "\n",
    "\n",
    "필수 요소로는 in_channels, out_channels,kernel_size 가있다. \n",
    "\n",
    "\n",
    "in_channels(int) : input image의 channel수 이다. rgb이미지라면 3이 되겠다.\n",
    "\n",
    "\n",
    "out_channels(int) : convolution에 의해서 생성된 channel의 수 이다. -> output의 픽셀 가로 세로가 아님.\n",
    "\n",
    "\n",
    "kernel_size(int or tuple) : convoling_kenel 의 크기이다. 보통은 filter라고 부르는 것과 동일하다.\n",
    "\n",
    "\n",
    "stride(int or tuple) : convolution의 stride를 얼만큼 줄 것이가 이다. Default는 1이다.\n",
    "\n",
    "\n",
    "padding(int or tuple) : zero padding을 input의 양쪽에 인자 만큼 해준다. Default는 0이라서 기본적으로 설정해주지 않으면 zero padding은 하지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 6, 5) # input channels, output channels, kernel size\n",
    "#         self.pool = nn.MaxPool2d(2, 2)  # kernel size, stride, padding = 0 (default)\n",
    "#         self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "#         self.fc1 = nn.Linear(16 * 5 * 5, 120) # input features, output features\n",
    "#         self.fc2 = nn.Linear(120, 84)\n",
    "#         self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "      # 항상 torch.nn.Module을 상속받고 시작\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5, 1) # 6@24*24 24가 어디서 나왔을까? mnist data의 한 이미지 크기는 28. 위의 공식 대입하여(28-5)/1+!=24, #채널을 6개로 만듬\n",
    "        # activation ReLU\n",
    "        self.pool1 = nn.MaxPool2d(2) # 6@12*12 2*2로 pooling 하면 이미지의 크기가 절반이 된다.\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, 1) # 16@8*8 #아까 채널이 6개니까 지금의 Input이 6\n",
    "        # activation ReLU\n",
    "        self.pool2 = nn.MaxPool2d(2) # 16@4*4 #pooling 2로 하면 이미지 크기가 절반이 된다.\n",
    "        \n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120) # input features, output features\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "num_epochs = 2\n",
    "num_batches = len(trainloader)\n",
    "\n",
    "\n",
    "trn_loss_list = []\n",
    "val_loss_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 0.009\n",
      "[1,   200] loss: 0.007\n",
      "[1,   300] loss: 0.007\n",
      "[1,   400] loss: 0.007\n",
      "[1,   500] loss: 0.006\n",
      "[1,   600] loss: 0.006\n",
      "[1,   700] loss: 0.005\n",
      "[1,   800] loss: 0.006\n",
      "[1,   900] loss: 0.006\n",
      "[2,   100] loss: 0.005\n",
      "[2,   200] loss: 0.005\n",
      "[2,   300] loss: 0.006\n",
      "[2,   400] loss: 0.005\n",
      "[2,   500] loss: 0.005\n",
      "[2,   600] loss: 0.005\n",
      "[2,   700] loss: 0.005\n",
      "[2,   800] loss: 0.005\n",
      "[2,   900] loss: 0.005\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):  # 데이터셋을 수차례 반복합니다.\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # 입력을 받은 후\n",
    "        inputs, labels = data\n",
    "\n",
    "        # 변화도(Gradient) 매개변수를 0으로 만든 후\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 순전파 + 역전파 + 최적화\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 통계 출력\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 100 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
